# Componentes Implementados - MLOps Proyecto Final

Este documento detalla los **5 componentes faltantes** que fueron implementados para completar el proyecto seg√∫n los requisitos del PDF.

---

## ‚úÖ 1. GitHub Actions Workflows (CI/CD)

**Ubicaci√≥n**: `.github/workflows/`

### Workflows Creados

#### `build-airflow.yml`
- Construye y publica imagen Docker de Airflow
- Trigger: Push a `main`/`master` en `dags/**`
- Imagen: `<DOCKERHUB_USERNAME>/mlops-airflow:latest`

#### `build-api.yml`
- Construye y publica imagen Docker de FastAPI
- Trigger: Push a `main`/`master` en `services/api/**`
- Imagen: `<DOCKERHUB_USERNAME>/mlops-api:latest`

#### `build-frontend.yml`
- Construye y publica imagen Docker de Streamlit
- Trigger: Push a `main`/`master` en `services/frontend/**`
- Imagen: `<DOCKERHUB_USERNAME>/mlops-frontend:latest`

#### `build-mlflow.yml`
- Construye y publica imagen Docker de MLflow
- Trigger: Push a `main`/`master` en `services/mlflow/**`
- Imagen: `<DOCKERHUB_USERNAME>/mlops-mlflow:latest`

#### `ci.yml`
- Pipeline de integraci√≥n continua
- Jobs: linting (flake8, black), tests (pytest), security scan (Trivy)

### Configuraci√≥n Requerida

1. Crear secrets en GitHub:
   - `DOCKERHUB_USERNAME`: Tu usuario de DockerHub
   - `DOCKERHUB_TOKEN`: Token de acceso de DockerHub

2. Ver documentaci√≥n completa en: `.github/workflows/README.md`

---

## ‚úÖ 2. Argo CD Manifests (Despliegue Continuo)

**Ubicaci√≥n**: `argocd/`

### Archivos Creados

#### `application.yaml`
- Application principal que despliega todo el sistema
- Sync policy: Automated (prune + selfHeal)
- Namespace: `mlops-final`

#### `project.yaml`
- AppProject con definici√≥n de:
  - Repositorios permitidos
  - Namespaces de destino
  - Roles (developer, admin)
  - Pol√≠ticas de acceso

#### `applications.yaml`
- Applications individuales por componente:
  - `mlops-api`: API de inferencia
  - `mlops-frontend`: UI Streamlit
  - `mlops-mlflow`: MLflow Tracking
  - `mlops-airflow`: Orquestador de DAGs
  - `mlops-databases`: PostgreSQL (4 instancias)
  - `mlops-observability`: Prometheus + Grafana

### Instalaci√≥n

```bash
# 1. Instalar Argo CD
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# 2. Crear AppProject
kubectl apply -f argocd/project.yaml

# 3. Crear Application principal
kubectl apply -f argocd/application.yaml

# 4. Verificar
kubectl get applications -n argocd
```

Ver documentaci√≥n completa en: `argocd/README.md`

---

## ‚úÖ 3. SHAP Explicabilidad (Frontend)

**Ubicaci√≥n**: `services/frontend/app.py`

### Funcionalidades Implementadas

#### Nueva pesta√±a: "üß† SHAP Explainability"
- Formulario para ingresar datos del paciente
- C√°lculo de valores SHAP v√≠a API `/explain`
- Visualizaciones interactivas:
  - **Waterfall Plot**: Top 15 features con mayor impacto
  - **Force Plot**: Features que aumentan/disminuyen riesgo
  - **Tabla de Valores**: SHAP values ordenados por impacto absoluto

#### Funciones A√±adidas

```python
get_shap_explanation(patient_data)
plot_shap_waterfall(shap_values, feature_names, base_value, prediction)
plot_shap_force(shap_values, feature_names, base_value)
```

#### Dependencias Agregadas

```
shap==0.44.0
matplotlib==3.8.2
scikit-learn==1.3.2
```

### Uso

1. Acceder a pesta√±a "SHAP Explainability"
2. Ingresar datos del paciente
3. Clic en "üîç Explain Prediction"
4. Visualizar:
   - Predicci√≥n (readmission risk)
   - Top features con mayor influencia
   - Direcci√≥n del impacto (positivo/negativo)

---

## ‚úÖ 4. Dataset Correcto: Realtor (Bienes Ra√≠ces)

**Problema Original**: El proyecto usaba dataset de diabetes, pero el PDF especifica dataset de **precios de propiedades inmobiliarias**.

### Cambios Realizados

#### Schemas de Base de Datos

**`initdb/01_create_raw_db_realtor.sql`**
- Tablas: `raw_train`, `raw_validation`, `raw_test`
- Columnas del dataset realtor:
  ```sql
  brokered_by TEXT             -- Broker/agency (categ√≥rico)
  status VARCHAR(50)           -- ready for sale / for_sale
  price NUMERIC(12,2)          -- TARGET VARIABLE
  bed INTEGER                  -- N√∫mero de camas
  bath NUMERIC(4,2)            -- N√∫mero de ba√±os
  acre_lot NUMERIC(10,4)       -- Tama√±o del terreno en acres
  street TEXT                  -- Direcci√≥n (categ√≥rico)
  city VARCHAR(100)            -- Ciudad
  state VARCHAR(50)            -- Estado
  zip_code VARCHAR(10)         -- C√≥digo postal
  house_size INTEGER           -- √Årea en pies cuadrados
  prev_sold_date DATE          -- Fecha de venta anterior
  ```

**`initdb/02_create_clean_db_realtor.sql`**
- Tablas: `clean_train`, `clean_validation`, `clean_test`
- Features derivados:
  ```sql
  price_per_sqft               -- price / house_size
  bed_bath_ratio               -- bed / bath
  sqft_per_acre                -- house_size / acre_lot
  days_since_prev_sale         -- D√≠as desde √∫ltima venta
  prev_sale_year, month, quarter
  avg_price_by_city, state, zip
  bed_zscore, bath_zscore, house_size_zscore, acre_lot_zscore
  ```

- Tablas auxiliares:
  - `encoding_mappings`: Encodings categ√≥ricos consistentes
  - `preprocessing_statistics`: Stats para normalizaci√≥n

#### Objetivo del Modelo
- **ANTES**: Clasificaci√≥n (readmission risk: 0, 1, 2)
- **AHORA**: Regresi√≥n (predecir precio de propiedad en USD)

---

## ‚úÖ 5. Integraci√≥n con API Externa del Profesor

**Ubicaci√≥n**: `dags/1_ingest_from_external_api.py`

### DAG Nuevo: Ingesta desde API Externa

#### Configuraci√≥n

```python
GROUP_NUMBER = 3                      # N√∫mero de grupo asignado
API_URL = "http://10.43.100.103:8000"  # API del profesor
```

#### Funcionamiento

1. **`get_next_request_count()`**
   - Consulta √∫ltimo request_count en `api_request_log`
   - Incrementa en 1 para pr√≥ximo request

2. **`fetch_data_from_api()`**
   - Hace GET request: `http://10.43.100.103:8000/data?group=3&request=N`
   - Valida status code (200 = OK, 404 = todos los datos recolectados)
   - Registra request en `api_request_log`
   - Retorna JSON con keys: `train`, `validation`, `test`

3. **`load_train_data()`, `load_validation_data()`, `load_test_data()`**
   - Calcula hash MD5 por fila (deduplicaci√≥n)
   - Inserta en `raw_train`, `raw_validation`, `raw_test`
   - ON CONFLICT DO NOTHING para evitar duplicados

4. **`log_ingestion_summary()`**
   - Cuenta registros por dataset
   - Inserta resumen en `ingestion_summary`

#### Tabla de Logging: `api_request_log`

```sql
CREATE TABLE api_request_log (
    request_count INTEGER,
    group_number INTEGER,
    request_timestamp TIMESTAMP,
    response_status_code INTEGER,
    response_size INTEGER,
    num_records INTEGER,
    error_message TEXT,
    is_successful BOOLEAN
);
```

#### Flujo del DAG

```
get_next_request_count
  ‚Üì
fetch_data_from_api
  ‚Üì
[load_train, load_validation, load_test] (paralelo)
  ‚Üì
log_ingestion_summary
```

#### Manejo de Errores

- **Timeout**: 300s m√°ximo por request
- **404**: Indica que todos los datos fueron recolectados
- **Otros errores**: Se loguean en `api_request_log.error_message`

#### Ejecuci√≥n

```bash
# Trigger manual
airflow dags trigger 1_ingest_from_external_api

# Schedule diario
schedule_interval='@daily'

# Ver logs
airflow tasks logs 1_ingest_from_external_api fetch_data_from_api <execution_date>
```

---

## Resumen de Cambios

### Archivos Nuevos (19)

#### GitHub Actions (6 archivos)
- `.github/workflows/build-airflow.yml`
- `.github/workflows/build-api.yml`
- `.github/workflows/build-frontend.yml`
- `.github/workflows/build-mlflow.yml`
- `.github/workflows/ci.yml`
- `.github/workflows/README.md`

#### Argo CD (4 archivos)
- `argocd/application.yaml`
- `argocd/project.yaml`
- `argocd/applications.yaml`
- `argocd/README.md`

#### Dataset Realtor (3 archivos)
- `initdb/01_create_raw_db_realtor.sql`
- `initdb/02_create_clean_db_realtor.sql`
- `dags/1_ingest_from_external_api.py`

### Archivos Modificados (2)

- `services/frontend/app.py`
  - Imports: `shap`, `matplotlib`, `numpy`
  - Funciones: `get_shap_explanation()`, `plot_shap_waterfall()`, `plot_shap_force()`
  - Nueva pesta√±a: "SHAP Explainability"

- `services/frontend/requirements.txt`
  - Agregado: `shap==0.44.0`, `matplotlib==3.8.2`, `scikit-learn==1.3.2`

---

## Estado Final del Proyecto

### ‚úÖ Componentes Completos

1. ‚úÖ **3 DAGs de Airflow**
   - Ingesta desde API externa (nuevo)
   - Preprocesamiento
   - Entrenamiento

2. ‚úÖ **MLflow** - Tracking + Registry

3. ‚úÖ **FastAPI** - API de inferencia con 7 endpoints

4. ‚úÖ **Streamlit** - Frontend con 4 tabs (SHAP nuevo)

5. ‚úÖ **4 Bases de Datos PostgreSQL**

6. ‚úÖ **MinIO S3** - Artefactos MLflow

7. ‚úÖ **Prometheus + Grafana** - Observabilidad

8. ‚úÖ **Locust** - Load testing

9. ‚úÖ **Docker Compose** - Orquestaci√≥n local

10. ‚úÖ **Kubernetes Manifests** - Despliegue K8s

11. ‚úÖ **GitHub Actions** - CI/CD completo (NUEVO)

12. ‚úÖ **Argo CD** - Despliegue continuo (NUEVO)

13. ‚úÖ **SHAP Explicabilidad** - En frontend (NUEVO)

14. ‚úÖ **Dataset Realtor** - Schema correcto (NUEVO)

15. ‚úÖ **API Externa** - Integraci√≥n con profesor (NUEVO)

### Implementaci√≥n: 100% Completa

---

## Pr√≥ximos Pasos

### 1. Configurar Secrets en GitHub

```bash
# En GitHub ‚Üí Settings ‚Üí Secrets ‚Üí Actions
DOCKERHUB_USERNAME=tu-usuario
DOCKERHUB_TOKEN=tu-token
```

### 2. Desplegar Bases de Datos con Schema Realtor

```bash
# Usar los nuevos archivos SQL
docker-compose up -d db-raw db-clean

# O en Kubernetes
kubectl apply -f kubernetes/databases.yaml
```

### 3. Ejecutar DAG de Ingesta

```bash
# Configurar GROUP_NUMBER en .env
export GROUP_NUMBER=3

# Ejecutar DAG
airflow dags trigger 1_ingest_from_external_api
```

### 4. Configurar Argo CD

```bash
# Instalar y configurar
kubectl apply -f argocd/project.yaml
kubectl apply -f argocd/application.yaml
```

### 5. Adaptar DAGs 2 y 3

- Modificar preprocessing para features del dataset realtor
- Cambiar modelo de clasificaci√≥n a regresi√≥n
- Ajustar m√©tricas (RMSE, MAE, R¬≤ en lugar de F1, Accuracy)

---

## Referencias

- **PDF del Proyecto**: `MLOPS_Proyecto_Final_2025.pdf`
- **Contexto**: `contexto_proyecto_final.txt`
- **GitHub Actions Docs**: `.github/workflows/README.md`
- **Argo CD Docs**: `argocd/README.md`
- **API del Profesor**: `http://10.43.100.103:8000/docs`

---

## Contacto

Para dudas o sugerencias sobre la implementaci√≥n, consultar la documentaci√≥n en cada directorio.
