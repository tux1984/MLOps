# Penguins ML API

## ğŸ” DescripciÃ³n general

**Penguins ML API** es una aplicaciÃ³n construida con **FastAPI** que entrena un modelo de Machine Learning para predecir la especie de un pingÃ¼ino usando el dataset [Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/).

El flujo de la app es:

1. **Entrenamiento** de un modelo de clasificaciÃ³n en `scripts/train.py`.
2. **SerializaciÃ³n** del modelo entrenado en `artifacts/model.joblib`.
3. **ExposiciÃ³n** del modelo vÃ­a API REST con **FastAPI** (`app/main.py`).
4. **Despliegue** en contenedor Docker.

---

## ğŸ—‚ï¸ Estructura del proyecto

```
NIVEL_0/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuraciones generales
â”‚   â”‚   â””â”€â”€ utils.py           # Utilidades para cargar el modelo
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py         # Modelos Pydantic para validaciÃ³n de datos
â”‚   â””â”€â”€ main.py                # DefiniciÃ³n de la API FastAPI
â”œâ”€â”€ artifacts/
â”‚   â””â”€â”€ model.joblib           # Modelo entrenado (se genera al entrenar)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train.py               # Script de entrenamiento (Data Prep + Model Creation)
â”œâ”€â”€ requirements.txt           # Dependencias de Python
â”œâ”€â”€ Dockerfile                 # DefiniciÃ³n de la imagen Docker
â””â”€â”€ README.md                  # Este archivo
```

---

## ğŸ“ Flujo de entrenamiento (`scripts/train.py`)

### **1. Data preparation**

- **Carga**: se carga el dataset con `palmerpenguins.load_penguins()`.
- **Limpieza**: se eliminan columnas irrelevantes (como `year`) y nulos.
- **TransformaciÃ³n**: normaliza columnas categÃ³ricas (`sex`, `island`).
- **ValidaciÃ³n**: checks de valores positivos y categorÃ­as vÃ¡lidas.
- **IngenierÃ­a de CaracterÃ­sticas**: agrega `bill_ratio` = bill\_length / bill\_depth.
- **DivisiÃ³n**: train/test estratificado.

### **2. Model creation**

- **ConstrucciÃ³n**: `Pipeline` con `ColumnTransformer` para preprocessing.
- **Entrenamiento**: Logistic Regression multinomial.
- **ValidaciÃ³n**: `accuracy_score` y `classification_report`.

### **3. Salida**

- Guarda `artifacts/model.joblib` con:
  - `pipeline`: el pipeline completo.
  - `target_names`: clases.
  - `metrics`: mÃ©tricas de validaciÃ³n.

---

## ğŸ“Š API con FastAPI (`app/main.py`)

### Endpoints

#### **GET** `/health`

Verifica el estado de la API y la versiÃ³n.

**Respuesta ejemplo:**

```json
{
  "status": "ok",
  "version": "0.1.0"
}
```

#### **POST** `/predict`

Predice la especie de uno o mÃ¡s pingÃ¼inos.

**Request Body:**

```json
{
  "items": [
    {
      "island": "Biscoe",
      "bill_length_mm": 39.1,
      "bill_depth_mm": 18.7,
      "flipper_length_mm": 181,
      "body_mass_g": 3750,
      "sex": "male",
      "bill_ratio": 2.1
    }
  ]
}
```

**Respuesta ejemplo:**

```json
{
  "predictions": [
    {
      "species": "Adelie",
      "probabilities": {
        "Adelie": 0.85,
        "Chinstrap": 0.10,
        "Gentoo": 0.05
      }
    }
  ]
}
```

---

## ğŸ› ï¸ EjecuciÃ³n local

### 1) Crear entorno y dependencias

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

### 2) Entrenar el modelo

```bash
python scripts/train.py
```

### 3) Levantar la API

```bash
uvicorn app.main:app --reload --port 8989
```

DocumentaciÃ³n interactiva en:

```
http://localhost:8989/docs
```

---

## ğŸ“¦ Construir y correr con Docker

### 1) Entrenar localmente

```bash
python scripts/train.py
```

Esto genera `artifacts/model.joblib`.

### 2) Construir imagen

```bash
docker build -t penguins-api .
```

### 3) Ejecutar contenedor

```bash
docker run -p 8989:8989 penguins-api
```

La API estarÃ¡ disponible en:

```
http://localhost:8989/docs
```

---

## ğŸ”— Referencias

- Dataset: [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/)
- Framework: [FastAPI](https://fastapi.tiangolo.com/)
- ML: [scikit-learn](https://scikit-learn.org/stable/)

---

## ğŸ’¡ Notas finales

- La ingenierÃ­a de features (`bill_ratio`) se realiza dentro del pipeline para evitar **train/serve skew**.
- El pipeline estÃ¡ diseÃ±ado para recibir datos crudos y procesarlos de forma consistente tanto en entrenamiento como en inferencia.
- El contenedor usa un usuario no-root y una imagen `python:3.12-slim` para ser mÃ¡s seguro y liviano.

